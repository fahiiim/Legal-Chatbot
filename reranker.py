"""
Cross-Encoder Reranker
Implements advanced re-ranking to improve retrieval relevance by 15-30%.
"""

from typing import List, Tuple, Optional
import numpy as np

try:
    from langchain.schema import Document
except ImportError:
    from langchain_core.documents import Document

try:
    from sentence_transformers import CrossEncoder
    CROSS_ENCODER_AVAILABLE = True
except ImportError:
    CROSS_ENCODER_AVAILABLE = False
    print("Warning: sentence-transformers not installed. Reranker will use fallback scoring.")


class DocumentReranker:
    """
    Cross-encoder based reranker for improving retrieval quality.
    
    Cross-encoders process query and document together, providing more
    accurate relevance scores than bi-encoder similarity alone.
    """
    
    def __init__(self, 
                 model_name: str = "cross-encoder/ms-marco-MiniLM-L-12-v2",
                 use_reranker: bool = True,
                 batch_size: int = 32):
        """
        Initialize the reranker.
        
        Args:
            model_name: Cross-encoder model to use
            use_reranker: Whether to use reranker or fallback to score passthrough
            batch_size: Batch size for reranking
        """
        self.use_reranker = use_reranker and CROSS_ENCODER_AVAILABLE
        self.batch_size = batch_size
        self.model = None
        
        if self.use_reranker:
            try:
                print(f"Loading cross-encoder model: {model_name}")
                self.model = CrossEncoder(model_name, max_length=512)
                print("âœ“ Reranker model loaded successfully")
            except Exception as e:
                print(f"Warning: Could not load cross-encoder model: {e}")
                print("Falling back to score passthrough mode")
                self.use_reranker = False
        else:
            print("Reranker disabled or sentence-transformers not available")
    
    def rerank(self, 
               query: str, 
               documents: List[Document], 
               top_k: Optional[int] = None,
               return_scores: bool = False) -> List[Document] | List[Tuple[Document, float]]:
        """
        Rerank documents based on query relevance.
        
        Args:
            query: Search query
            documents: List of retrieved documents to rerank
            top_k: Number of top documents to return (None = all)
            return_scores: Whether to return scores with documents
        
        Returns:
            Reranked list of documents, optionally with scores
        """
        if not documents:
            return []
        
        # If reranker not available, return documents as-is
        if not self.use_reranker:
            if top_k:
                documents = documents[:top_k]
            if return_scores:
                return [(doc, 1.0) for doc in documents]
            return documents
        
        # Prepare query-document pairs
        pairs = [[query, doc.page_content[:2000]] for doc in documents]  # Truncate long docs
        
        # Get cross-encoder scores
        try:
            scores = self.model.predict(pairs, batch_size=self.batch_size)
        except Exception as e:
            print(f"Error during reranking: {e}")
            # Fallback to original order
            if return_scores:
                return [(doc, 1.0) for doc in documents]
            return documents
        
        # Combine documents with scores
        doc_score_pairs = list(zip(documents, scores))
        
        # Sort by score (descending)
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # Apply top_k filter
        if top_k:
            doc_score_pairs = doc_score_pairs[:top_k]
        
        if return_scores:
            return doc_score_pairs
        else:
            return [doc for doc, _ in doc_score_pairs]
    
    def rerank_with_metadata(self,
                            query: str,
                            documents: List[Document],
                            top_k: Optional[int] = None) -> Tuple[List[Document], dict]:
        """
        Rerank and return metadata about the reranking process.
        
        Args:
            query: Search query
            documents: Documents to rerank
            top_k: Number of top documents to return
        
        Returns:
            Tuple of (reranked_documents, metadata_dict)
        """
        if not documents:
            return [], {"num_documents": 0, "reranking_applied": False}
        
        original_order = [i for i in range(len(documents))]
        doc_score_pairs = self.rerank(query, documents, top_k=top_k, return_scores=True)
        
        reranked_docs = [doc for doc, _ in doc_score_pairs]
        scores = [score for _, score in doc_score_pairs]
        
        # Calculate metadata
        metadata = {
            "num_documents": len(reranked_docs),
            "reranking_applied": self.use_reranker,
            "scores": {
                "mean": float(np.mean(scores)) if scores else 0.0,
                "max": float(np.max(scores)) if scores else 0.0,
                "min": float(np.min(scores)) if scores else 0.0,
                "std": float(np.std(scores)) if scores else 0.0
            }
        }
        
        # Check if order changed significantly
        if self.use_reranker and len(documents) > 1:
            # Count position changes
            position_changes = sum(1 for i, doc in enumerate(reranked_docs) 
                                 if documents.index(doc) != i)
            metadata["position_changes"] = position_changes
            metadata["order_changed"] = position_changes > 0
        
        return reranked_docs, metadata
    
    def batch_rerank(self,
                     queries: List[str],
                     document_lists: List[List[Document]],
                     top_k: Optional[int] = None) -> List[List[Document]]:
        """
        Rerank multiple query-document sets in batch.
        
        Args:
            queries: List of queries
            document_lists: List of document lists (one per query)
            top_k: Number of top documents to return per query
        
        Returns:
            List of reranked document lists
        """
        results = []
        for query, docs in zip(queries, document_lists):
            reranked = self.rerank(query, docs, top_k=top_k)
            results.append(reranked)
        return results
    
    def get_relevance_scores(self, query: str, documents: List[Document]) -> List[float]:
        """
        Get relevance scores for documents without reranking.
        
        Args:
            query: Search query
            documents: Documents to score
        
        Returns:
            List of relevance scores
        """
        if not self.use_reranker or not documents:
            return [1.0] * len(documents)
        
        pairs = [[query, doc.page_content[:2000]] for doc in documents]
        scores = self.model.predict(pairs, batch_size=self.batch_size)
        return scores.tolist()


class LegalReranker(DocumentReranker):
    """
    Legal-domain optimized reranker.
    Applies domain-specific boosting for legal citations and terminology.
    """
    
    def __init__(self, 
                 model_name: str = "cross-encoder/ms-marco-MiniLM-L-12-v2",
                 use_reranker: bool = True,
                 citation_boost: float = 0.1):
        """
        Initialize legal reranker.
        
        Args:
            model_name: Cross-encoder model
            use_reranker: Whether to use reranking
            citation_boost: Score boost for documents with legal citations
        """
        super().__init__(model_name, use_reranker)
        self.citation_boost = citation_boost
        
        # Legal citation patterns
        import re
        self.citation_patterns = [
            re.compile(r'MCR\s+\d+\.\d+', re.IGNORECASE),
            re.compile(r'Fed\.\s*R\.\s*(?:Civ\.|Crim\.)\s*P\.\s*\d+', re.IGNORECASE),
            re.compile(r'Fed\.\s*R\.\s*Evid\.\s*\d+', re.IGNORECASE),
            re.compile(r'M\s+(?:Civ|Crim)\s+JI\s+\d+\.\d+', re.IGNORECASE),
        ]
    
    def _has_legal_citations(self, text: str) -> bool:
        """Check if text contains legal citations."""
        for pattern in self.citation_patterns:
            if pattern.search(text):
                return True
        return False
    
    def rerank(self, 
               query: str, 
               documents: List[Document], 
               top_k: Optional[int] = None,
               return_scores: bool = False) -> List[Document] | List[Tuple[Document, float]]:
        """
        Rerank with legal citation boosting.
        
        Args:
            query: Search query
            documents: Documents to rerank
            top_k: Number of top documents
            return_scores: Whether to return scores
        
        Returns:
            Reranked documents with optional scores
        """
        # Get base scores
        doc_score_pairs = super().rerank(query, documents, top_k=None, return_scores=True)
        
        # Apply citation boost
        boosted_pairs = []
        for doc, score in doc_score_pairs:
            if self._has_legal_citations(doc.page_content):
                boosted_score = score + self.citation_boost
            else:
                boosted_score = score
            boosted_pairs.append((doc, boosted_score))
        
        # Re-sort with boosted scores
        boosted_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # Apply top_k
        if top_k:
            boosted_pairs = boosted_pairs[:top_k]
        
        if return_scores:
            return boosted_pairs
        else:
            return [doc for doc, _ in boosted_pairs]
